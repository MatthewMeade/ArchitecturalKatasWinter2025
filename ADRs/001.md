# Usage limits for candidate's test responses by LLM / AI 

## Status

Proposed

## Context

LLM based apps can be vulnerable to attacks carried out by carefully crafting inputs or prompts.
These attacks, known as prompt hacking, can be used to trick LLMs based apps into generating unintended or malicious output. Source: https://owasp.org/www-project-llm-prompt-hacking

## Decision

This ADR is proposing to define and limit the usage scope of the candidate's test responses by an LLM.
A candidate's test responses can be used directly as LLM input if we can ensure:
- The responses will not affect future LLM responses beyond the scope of the candidate's test grading.
- The responses will not be used unscrutinized / unsanitized as a training input for the LLM affecting future LLM responses.

The LLM's output after any candidate's test responses input, should not be available to the candidate but only to the grading consultant. 

## Consequences

Future LLM training will become better, since we would have more control of the training input data set.
There needs to be a mechanism to choose if a candidate's response can be used for future LLM training.