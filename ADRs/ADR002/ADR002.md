
# Train LLMs using grading context

## Status

proposed

## Context

The default LLM models do not carry suffucient context to help grade tests efficiently and effectively.

Therefore the LLM models will need to have a way of consuming this context and provide answers considering the consumed context.

It is understood that consuming all the context, which might include relevant documentation, previoius responses or other data is time consuming and therefore costly.

Therefore it can not happen again and again every time there is a need to interact with the LLM to get a grade suggestion.

That means that the LLM needs to save it's state just after consuming the context, and before it receives the candidate's responses to be graded.

Feeding it context amd explicitly saving it for future reference falls under the training category itself, so you might see training refering to the LLM consuming context relevant for grading and saving it for future reference.


## Decision

Use available LLMs, feed it relevant context, and save its state as it is ready to receive the candidate's response to be graded.

## Consequences

This also ensures that all candidates will use the same LLM that understood the same context, so all candidates will have an equal starting point ensuring grading fairnes.

This also introduces the opportunity to version control the LLMs. Version control allows to test-before-release the next LLM after consuming some new context, and roll back any LLM states that produce undesireable or harmful to the company results.
